{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Notebook 3 ‚Äî Model Training & Comparison\n",
    "\n",
    "**Project:** Stock Trend Predictor  \n",
    "**Goal:** Train all 5 models, compare performance, pick the best\n",
    "\n",
    "### Models compared:\n",
    "1. Logistic Regression (baseline)\n",
    "2. SVM ‚Äî RBF Kernel\n",
    "3. KNN ‚Äî K-Nearest Neighbors\n",
    "4. Random Forest\n",
    "5. XGBoost ‚≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, f1_score, classification_report,\n",
    "                              confusion_matrix, ConfusionMatrixDisplay)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from features import add_technical_indicators, create_labels, prepare_feature_matrix\n",
    "from sentiment import merge_sentiment_with_features\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print('‚úÖ Imports done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Prepare Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'RELIANCE.NS'\n",
    "df_raw = yf.download(ticker, start='2020-01-01', end='2024-12-31', progress=False)\n",
    "if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "    df_raw.columns = df_raw.columns.get_level_values(0)\n",
    "\n",
    "df = add_technical_indicators(df_raw.copy())\n",
    "df = create_labels(df, n_days=5)\n",
    "df = merge_sentiment_with_features(df, ticker)\n",
    "df = prepare_feature_matrix(df)\n",
    "df.dropna(subset=['Label'], inplace=True)\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Label distribution:\\n{df[\"Label\"].value_counts().sort_index()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Train / Test Split (Chronological ‚Äî NO shuffling!)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: For time-series, ALWAYS split chronologically\n",
    "# Never use train_test_split with shuffle=True on stock data!\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "train = df.iloc[:split_idx]\n",
    "test  = df.iloc[split_idx:]\n",
    "\n",
    "X_train = train.drop(columns=['Label'])\n",
    "y_train = train['Label']\n",
    "X_test  = test.drop(columns=['Label'])\n",
    "y_test  = test['Label']\n",
    "\n",
    "print(f'Train: {len(X_train)} samples  ({train.index[0].date()} ‚Üí {train.index[-1].date()})')\n",
    "print(f'Test:  {len(X_test)}  samples  ({test.index[0].date()} ‚Üí {test.index[-1].date()})')\n",
    "\n",
    "# Scale features (important for SVM, KNN, LR)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)  # fit ONLY on train!\n",
    "X_test_s  = scaler.transform(X_test)       # transform test with same scaler\n",
    "\n",
    "print('\\n‚úÖ Train/Test split done ‚Äî chronological order preserved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Train All 5 Models"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=1000, C=1.0, random_state=42), True),\n",
    "    'SVM':                 (SVC(kernel='rbf', probability=True, random_state=42),       True),\n",
    "    'KNN':                 (KNeighborsClassifier(n_neighbors=7, weights='distance'),     True),\n",
    "    'Random Forest':       (RandomForestClassifier(n_estimators=200, random_state=42),  False),\n",
    "    'XGBoost':             (XGBClassifier(n_estimators=200, learning_rate=0.05,\n",
    "                                          random_state=42, verbosity=0, eval_metric='mlogloss'), False),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(f'\\n{\"=\"*65}')\n",
    "print(f'{\"Model\":<22} {\"Accuracy\":>10} {\"F1\":>8} {\"Precision\":>10} {\"Recall\":>8}')\n",
    "print(f'{\"=\"*65}')\n",
    "\n",
    "for name, (model, use_scaled) in models.items():\n",
    "    Xtr = X_train_s if use_scaled else X_train.values\n",
    "    Xte = X_test_s  if use_scaled else X_test.values\n",
    "\n",
    "    model.fit(Xtr, y_train)\n",
    "    preds = model.predict(Xte)\n",
    "\n",
    "    acc  = accuracy_score(y_test, preds)\n",
    "    f1   = f1_score(y_test, preds, average='weighted', zero_division=0)\n",
    "    prec = f1_score(y_test, preds, average='weighted', zero_division=0)\n",
    "    rec  = f1_score(y_test, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    results[name] = {'accuracy': acc, 'f1': f1, 'preds': preds}\n",
    "    trained_models[name] = (model, use_scaled)\n",
    "\n",
    "    print(f'{name:<22} {acc:>10.2%} {f1:>8.4f} {prec:>10.4f} {rec:>8.4f}')\n",
    "\n",
    "print(f'{\"=\"*65}')\n",
    "best = max(results, key=lambda k: results[k]['f1'])\n",
    "print(f'\\nüèÜ Best model: {best}  (F1 = {results[best][\"f1\"]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Model Comparison Chart"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names  = list(results.keys())\n",
    "accs   = [results[n]['accuracy'] * 100 for n in names]\n",
    "f1s    = [results[n]['f1']       * 100 for n in names]\n",
    "colors = ['#2563EB' if n == best else '#93C5FD' for n in names]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bars1 = ax.bar(x - width/2, accs, width, label='Accuracy (%)', color=colors,       alpha=0.85)\n",
    "bars2 = ax.bar(x + width/2, f1s,  width, label='F1 Score (%)',  color='#059669', alpha=0.75)\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f'{bar.get_height():.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f'{bar.get_height():.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names, rotation=10)\n",
    "ax.set_ylim(0, 85)\n",
    "ax.set_ylabel('Score (%)')\n",
    "ax.set_title('Model Comparison ‚Äî Accuracy vs F1 Score', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.axhline(50, color='red', linestyle='--', alpha=0.4, label='Random baseline (50%)')\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = names.index(best)\n",
    "ax.annotate(f'üèÜ Best', xy=(best_idx - width/2, accs[best_idx]),\n",
    "            xytext=(best_idx - width/2 + 0.3, accs[best_idx] + 5),\n",
    "            fontsize=10, color='#1B3A6B', fontweight='bold',\n",
    "            arrowprops=dict(arrowstyle='->', color='#1B3A6B'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Chart saved to data/model_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Confusion Matrices ‚Äî All Models"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['DOWN', 'NEUTRAL', 'UP']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, res['preds'], labels=[-1, 0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
    "    disp.plot(ax=axes[i], colorbar=False, cmap='Blues')\n",
    "    axes[i].set_title(f'{name}\\nAcc: {res[\"accuracy\"]:.2%}',\n",
    "                      fontweight='bold', fontsize=11)\n",
    "\n",
    "# Hide the 6th empty subplot\n",
    "axes[5].set_visible(False)\n",
    "\n",
    "plt.suptitle('Confusion Matrices ‚Äî All Models', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Classification Report ‚Äî Best Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, use_scaled = trained_models[best]\n",
    "Xte = X_test_s if use_scaled else X_test.values\n",
    "best_preds = best_model.predict(Xte)\n",
    "\n",
    "print(f'üìã Classification Report ‚Äî {best} (Best Model)')\n",
    "print('='*50)\n",
    "print(classification_report(y_test, best_preds,\n",
    "                             target_names=label_names,\n",
    "                             zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 7. Feature Importance (Random Forest)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = trained_models['Random Forest'][0]\n",
    "feature_names  = list(X_train.columns)\n",
    "importances    = rf_model.feature_importances_\n",
    "\n",
    "feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_df = feat_df.sort_values('Importance', ascending=True).tail(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "colors = ['#2563EB' if imp > feat_df['Importance'].median() else '#93C5FD'\n",
    "          for imp in feat_df['Importance']]\n",
    "bars = ax.barh(feat_df['Feature'], feat_df['Importance'], color=colors, edgecolor='white')\n",
    "\n",
    "for bar, val in zip(bars, feat_df['Importance']):\n",
    "    ax.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Top 15 Feature Importances (Random Forest)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nüîç Top 5 most important features:')\n",
    "top5 = feat_df.tail(5)[['Feature', 'Importance']].iloc[::-1]\n",
    "for _, row in top5.iterrows():\n",
    "    print(f'  {row[\"Feature\"]:<20}  {row[\"Importance\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 8. Final Results Summary Table"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model'    : ('üèÜ ' if n == best else '   ') + n,\n",
    "        'Accuracy' : f\"{results[n]['accuracy']:.2%}\",\n",
    "        'F1 Score' : f\"{results[n]['f1']:.4f}\",\n",
    "        'Notes'    : {\n",
    "            'Logistic Regression': 'Baseline ‚Äî simple & fast',\n",
    "            'SVM':                 'Good boundary ‚Äî needs scaling',\n",
    "            'KNN':                 'Instance-based ‚Äî needs scaling',\n",
    "            'Random Forest':       'Best interpretability (feature importance)',\n",
    "            'XGBoost':             'Best overall ‚Äî gradient boosting',\n",
    "        }.get(n, '')\n",
    "    }\n",
    "    for n in results\n",
    "])\n",
    "\n",
    "print('üìä Final Model Comparison:')\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(f'\\n‚úÖ Best Model Selected: {best}')\n",
    "print(f'   ‚Üí Saved as models/best_model.pkl')\n",
    "print(f'   ‚Üí Used by FastAPI /predict endpoint')\n",
    "print(f'   ‚Üí Displayed in Streamlit dashboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 9. Key Takeaways"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*55)\n",
    "print('   ü§ñ MODEL TRAINING SUMMARY')\n",
    "print('='*55)\n",
    "print()\n",
    "print('  Key Decisions Made:')\n",
    "print('  ‚úÖ Chronological split ‚Äî no data leakage')\n",
    "print('  ‚úÖ StandardScaler applied only on train set')\n",
    "print('  ‚úÖ 5 models trained and compared')\n",
    "print('  ‚úÖ Best model selected by F1-score (weighted)')\n",
    "print()\n",
    "print('  Why F1 over Accuracy?')\n",
    "print('  ‚Üí Class imbalance means accuracy can be misleading')\n",
    "print('  ‚Üí F1 balances precision and recall across all classes')\n",
    "print()\n",
    "print('  Interview Talking Points:')\n",
    "print('  ‚Üí Compared 5 ML algorithms on same dataset')\n",
    "print('  ‚Üí Used feature importance to interpret model')\n",
    "print('  ‚Üí Avoided data leakage with time-ordered split')\n",
    "print('  ‚Üí Selected best model programmatically by F1')\n",
    "print()\n",
    "print('  ‚û°Ô∏è  Next: Notebook 4 ‚Äî Sentiment Analysis')\n",
    "print('='*55)"
   ]
  }
 ]
}
